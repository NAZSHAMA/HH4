# Module 4 — Vision-Language-Action (VLA)

Goal:
Show how LLMs and perception systems enable humanoid robots to follow natural language commands and act autonomously.

Chapters:
1) Voice-to-Action (Whisper + ROS 2)
   - Convert speech to text
   - Parse command into robot actions
   - Trigger ROS 2 movement and sensing tasks

2) Cognitive Planning with LLMs
   - Convert “Clean the room” into step-by-step actions
   - Integrate vision to identify targets
   - Execute actions in sequence on a simulated humanoid

Requirements:
- Brief explanations, diagrams, and minimal code
- 1 practical task per chapter (e.g., voice command → pick an object)
- Capstone alignment: full pipeline from voice input to successful manipulation

Outcome:
Students build a humanoid robot that understands language and performs real actions.
